# ç¼–ç å™¨æ³¨æ„åŠ›æ©ç çš„ç›®åœ°ï¼šä½¿æ‰¹æ¬¡ä¸­è¾ƒçŸ­è¯­å¥çš„å¡«å……éƒ¨åˆ†ä¸å‚ä¸æ³¨æ„åŠ›è®¡ç®—ã€‚
# æ¨¡å‹è®­ç»ƒé€šå¸¸æŒ‰æ‰¹æ¬¡è¿›è¡Œï¼ŒåŒä¸€æ‰¹æ¬¡ä¸­çš„è¯­å¥é•¿åº¦å¯èƒ½ä¸åŒï¼Œå› æ­¤éœ€è¦æŒ‰è¯­å¥æœ€å¤§é•¿åº¦å¯¹çŸ­è¯­å¥è¿›è¡Œ0å¡«å……ä»¥è¡¥é½é•¿åº¦ã€‚
# è¯­å¥å¡«å……éƒ¨åˆ†å±äºæ— æ•ˆä¿¡æ¯ï¼Œä¸åº”å‚ä¸å‰å‘ä¼ æ’­
#
# è§£ç å™¨æ³¨æ„åŠ›æ©ç ç›¸å¯¹äºç¼–ç å™¨ç•¥å¾®å¤æ‚ï¼Œä¸ä»…éœ€è¦å°†å¡«å……éƒ¨åˆ†å±è”½æ‰ï¼Œè¿˜éœ€è¦å¯¹å½“å‰åŠåç»­åºåˆ—è¿›è¡Œå±è”½ï¼ˆsubsequent_maskï¼‰ï¼Œ
# å³è§£ç å™¨åœ¨é¢„æµ‹å½“å‰æ—¶åˆ»å•è¯æ—¶ï¼Œä¸èƒ½çŸ¥é“å½“å‰åŠåç»­å•è¯å†…å®¹ï¼Œå› æ­¤æ³¨æ„åŠ›æ©ç éœ€è¦å°†å½“å‰æ—¶åˆ»ä¹‹åçš„æ³¨æ„åŠ›åˆ†æ•°å…¨éƒ¨ç½®ä¸º âˆ’âˆ ï¼Œ
# ç„¶åå†è®¡ç®— ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ ï¼Œé˜²æ­¢å‘ç”Ÿæ•°æ®æ³„éœ²ã€‚
# subsequent_maskçš„çŸ©é˜µå½¢å¼ä¸ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œåœ¨ä¸»å¯¹è§’çº¿å³ä¸Šä½ç½®å…¨éƒ¨ä¸ºFalse


import torch
import numpy as np
from model import config
from torch.autograd import Variable

def subsequent_mask(size):
    "Mask out subsequent positions."
    # è®¾å®šsubsequent_maskçŸ©é˜µçš„shape
    attn_shape = (1, size, size)
    # ç”Ÿæˆä¸€ä¸ªå³ä¸Šè§’(ä¸å«ä¸»å¯¹è§’çº¿)ä¸ºå…¨1ï¼Œå·¦ä¸‹è§’(å«ä¸»å¯¹è§’çº¿)ä¸ºå…¨0çš„subsequent_maskçŸ©é˜µ
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    # è¿”å›ä¸€ä¸ªå³ä¸Šè§’(ä¸å«ä¸»å¯¹è§’çº¿)ä¸ºå…¨Falseï¼Œå·¦ä¸‹è§’(å«ä¸»å¯¹è§’çº¿)ä¸ºå…¨Trueçš„subsequent_maskçŸ©é˜µ
    return torch.from_numpy(subsequent_mask) == 0


class Batch:
    """
    æ‰¹æ¬¡ç±»
        1. è¾“å…¥åºåˆ—ï¼ˆæºï¼‰
        2. è¾“å‡ºåºåˆ—ï¼ˆç›®æ ‡ï¼‰
        3. æ„é€ æ©ç 
    """

    def __init__(self, src, trg=None, pad=config.PAD):
        # å°†è¾“å…¥ã€è¾“å‡ºå•è¯idè¡¨ç¤ºçš„æ•°æ®è§„èŒƒæˆæ•´æ•°ç±»å‹
        src = torch.from_numpy(src).to(config.DEVICE).long()
        trg = torch.from_numpy(trg).to(config.DEVICE).long()
        self.src = src
        # å¯¹äºå½“å‰è¾“å…¥çš„è¯­å¥éç©ºéƒ¨åˆ†è¿›è¡Œåˆ¤æ–­ï¼Œboolåºåˆ—
        # å¹¶åœ¨seq lengthå‰é¢å¢åŠ ä¸€ç»´ï¼Œå½¢æˆç»´åº¦ä¸º 1Ã—seq length çš„çŸ©é˜µ
        self.src_mask = (src != pad).unsqueeze(-2)
        # å¦‚æœè¾“å‡ºç›®æ ‡ä¸ä¸ºç©ºï¼Œåˆ™éœ€è¦å¯¹è§£ç å™¨ä½¿ç”¨çš„ç›®æ ‡è¯­å¥è¿›è¡Œæ©ç 
        if trg is not None:
            # è§£ç å™¨ä½¿ç”¨çš„ç›®æ ‡è¾“å…¥éƒ¨åˆ†
            self.trg = trg[:, : -1]
            # è§£ç å™¨è®­ç»ƒæ—¶åº”é¢„æµ‹è¾“å‡ºçš„ç›®æ ‡ç»“æœ
            self.trg_y = trg[:, 1:]
            # å°†ç›®æ ‡è¾“å…¥éƒ¨åˆ†è¿›è¡Œæ³¨æ„åŠ›æ©ç 
            self.trg_mask = self.make_std_mask(self.trg, pad)
            # å°†åº”è¾“å‡ºçš„ç›®æ ‡ç»“æœä¸­å®é™…çš„è¯æ•°è¿›è¡Œç»Ÿè®¡
            self.ntokens = (self.trg_y != pad).data.sum()

    # æ©ç æ“ä½œ
    @staticmethod
    def make_std_mask(tgt, pad):
        "Create a mask to hide padding and future words."
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        return tgt_mask